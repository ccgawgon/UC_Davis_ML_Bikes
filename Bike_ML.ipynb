{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506f3a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Helper function\n",
    "def findMinMax(data):\n",
    "  q1 = data.quantile(0.25)\n",
    "  q3 = data.quantile(0.75)\n",
    "  min = q1 - 1.5 * (q3 - q1)\n",
    "  max = q3 + 1.5 * (q3 - q1)\n",
    "  return min, max\n",
    "\n",
    "# Function to help us visualize the outliers. Produce boxplot and print out outliers\n",
    "def getOutliers(data, features):\n",
    "  for i, feature in enumerate(features, 1):\n",
    "    plt.subplot(1,len(features),i)\n",
    "    data[[feature]].boxplot()\n",
    "    \n",
    "    min, max = findMinMax(data[feature])\n",
    "    outliers_lower = data[feature] < min\n",
    "    outliers_upper = data[feature] > max\n",
    "    \n",
    "    if outliers_lower.any():\n",
    "      print(feature, \"- Lower outliers:\\n\", data.loc[outliers_lower, feature])\n",
    "    if outliers_upper.any():\n",
    "      print(feature, \"- Upper outliers:\\n\", data.loc[outliers_upper, feature])\n",
    "\n",
    "  plt.show()\n",
    "  \n",
    "\n",
    "# return az set of data with outliers removed\n",
    "def removeOutliers(data, features):\n",
    "  removeIdx = pd.Series([False] * len(data))\n",
    "  for i, feature in enumerate(features):\n",
    "    min, max = findMinMax(data[feature])\n",
    "    outliers_lower = data[feature] < min\n",
    "    outliers_upper = data[feature] > max\n",
    "    \n",
    "    removeIdx = removeIdx | outliers_lower | outliers_upper\n",
    "\n",
    "  return data.loc[~removeIdx]\n",
    "  \n",
    "\n",
    "\n",
    "# Testing code\n",
    "data = pd.DataFrame(pd.read_csv('./SeoulBikeData.csv'))\n",
    "features = ['Rented Bike Count', 'Wind speed (m/s)']\n",
    "\n",
    "getOutliers(data, features)\n",
    "\n",
    "newData = removeOutliers(data, features)\n",
    "getOutliers(newData, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260e6a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.DataFrame(pd.read_csv('./SeoulBikeData.csv'))\n",
    "\n",
    "# Remove rows with non-functioning day / no bike rented\n",
    "functioningDay = data['Functioning Day'] == 'Yes'\n",
    "data = data.loc[functioningDay]\n",
    "\n",
    "# Dropping some features:\n",
    "# Date: can't process and we already have the holiday feature\n",
    "# Dew temp: not relevant\n",
    "# Functioning day: already process\n",
    "data = data.drop(columns=['Date', 'Dew point temperature', 'Functioning Day'])\n",
    "\n",
    "\n",
    "# One Hot Encode categorical features\n",
    "# Hour should be categorical too. Not sure how to handle it yet\n",
    "data = pd.get_dummies(data, columns=['Seasons'], dtype=int)\n",
    "data = pd.get_dummies(data, columns=['Holiday'], dtype=int, drop_first=True)\n",
    "\n",
    "\n",
    "getOutliers(data, ['Rented Bike Count', 'Temperature', 'Humidity', 'Wind speed', 'Visibility', 'Solar Radiation', 'Rainfall', 'Snowfall'])\n",
    "\n",
    "# It seems like every rainy or snowy days are counted as outliers because the weather is normal most of the time.\n",
    "# Therefore, not going to remove outliers for Rainfall and Snowfall\n",
    "\n",
    "# A lot of outliers for Solar Radiation. We can test this out with our models. For now, not removing outliers for this one\n",
    "\n",
    "# Around 150 outliers for Rented Bike Count and Wind Speed. Remove outliers for now\n",
    "\n",
    "data = removeOutliers(data, ['Rented Bike Count', 'Temperature', 'Humidity', 'Wind speed', 'Visibility'])\n",
    "data = data.reset_index(drop=True)\n",
    "print(data)\n",
    "\n",
    "\n",
    "# Splitting data between categorical and numericals set for standardization\n",
    "categoricalFeatures = ['Hour', 'Seasons_Autumn', 'Seasons_Spring', 'Seasons_Summer', 'Seasons_Winter', 'Holiday_No Holiday']\n",
    "numericalFeatures = ['Rented Bike Count', 'Temperature', 'Humidity', 'Wind speed', 'Visibility', 'Solar Radiation', 'Rainfall', 'Snowfall']\n",
    "categoricalValues = data[categoricalFeatures]\n",
    "standardizedData = data.drop(columns=categoricalFeatures)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(standardizedData)\n",
    "standardizedData = scaler.transform(standardizedData)\n",
    "standardizedData = pd.DataFrame(standardizedData)\n",
    "standardizedData.columns = numericalFeatures\n",
    "\n",
    "standardizedData = pd.concat([standardizedData, categoricalValues], axis=1)\n",
    "print(standardizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0aeef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# not sure if this function is totally necessary because it runs without it\n",
    "def bgd(X, y, learning_rate=0.01, batch_size=32, reg_para=0.01, epochs=100, clip_value=5.0):\n",
    "    \n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initialize weights with small random values\n",
    "    weights = np.random.randn(n_features) * 0.01\n",
    "    bias = 0\n",
    "    loss_history = []\n",
    "    \n",
    "    # Calculate number of batches\n",
    "    n_batches = int(np.ceil(n_samples / batch_size))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch in range(n_batches):\n",
    "            start_idx = batch * batch_size\n",
    "            end_idx = min((batch + 1) * batch_size, n_samples)\n",
    "            X_batch = X_shuffled[start_idx:end_idx]\n",
    "            y_batch = y_shuffled[start_idx:end_idx]\n",
    "            batch_size_actual = end_idx - start_idx\n",
    "            \n",
    "            # Forward pass with numerical stability\n",
    "            y_pred = np.clip(np.dot(X_batch, weights) + bias, -1e15, 1e15)\n",
    "            \n",
    "            # Calculate gradients with numerical stability\n",
    "            errors = y_pred - y_batch\n",
    "            dw = (1/batch_size_actual) * (np.dot(X_batch.T, errors) + reg_para * weights)\n",
    "            db = np.mean(errors)\n",
    "            \n",
    "            # Gradient clipping\n",
    "            dw = np.clip(dw, -clip_value, clip_value)\n",
    "            db = np.clip(db, -clip_value, clip_value)\n",
    "            \n",
    "            # Update weights and bias with smaller learning rate\n",
    "            weights -= learning_rate * dw\n",
    "            bias -= learning_rate * db\n",
    "            \n",
    "            # Calculate loss with numerical stability\n",
    "            squared_errors = np.clip(errors ** 2, 0, 1e15)\n",
    "            batch_loss = np.mean(squared_errors) + (reg_para/(2*batch_size_actual)) * np.sum(np.clip(weights ** 2, 0, 1e15))\n",
    "            epoch_loss += batch_loss\n",
    "            \n",
    "            # Check for NaN values\n",
    "            if np.isnan(batch_loss) or np.isnan(weights).any():\n",
    "                print(f\"NaN detected at epoch {epoch}, batch {batch}. Stopping training.\")\n",
    "                return weights, bias, loss_history\n",
    "        \n",
    "        epoch_loss /= n_batches\n",
    "        loss_history.append(epoch_loss)\n",
    "        \n",
    "        # Early stopping if loss is very small\n",
    "        if epoch > 0 and abs(loss_history[-1] - loss_history[-2]) < 1e-7:\n",
    "            break\n",
    "            \n",
    "    return weights, bias, loss_history\n",
    "\n",
    "\n",
    "def predict_batch_gd(X, weights, bias):\n",
    "    # prevent overflow\n",
    "    return np.clip(np.dot(X, weights) + bias, -1e15, 1e15)\n",
    "\n",
    "# batch gradient descent with l2 regularization\n",
    "def grid_search_bgd(X, y, folds=10, degrees=[1, 2], \n",
    "                   batch_sizes=[32, 64],\n",
    "                   learning_rates=[0.00001, 0.0001], \n",
    "                   reg_parameters=[0.00001, 0.0001]):\n",
    "    \n",
    "    # store best results\n",
    "    best_mse = float('inf')\n",
    "    best_params = None\n",
    "    best_weights = None\n",
    "    best_bias = None\n",
    "\n",
    "    # need to convert to numpy arrays for compatibility w sklearn\n",
    "    X_np = X.to_numpy() if hasattr(X, 'to_numpy') else X\n",
    "    y_np = y.to_numpy() if hasattr(y, 'to_numpy') else y\n",
    "    \n",
    "    # scale y to prevent overflow\n",
    "    y_scale = np.max(np.abs(y_np))\n",
    "    y_np = y_np / y_scale\n",
    "    \n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "    \n",
    "    for degree in degrees:\n",
    "        poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "        X_poly = poly.fit_transform(X_np)\n",
    "        \n",
    "        # scale X to prevent overflow\n",
    "        X_scale = np.max(np.abs(X_poly))\n",
    "        X_poly = X_poly / X_scale\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            for learning_rate in learning_rates:\n",
    "                for reg_para in reg_parameters:\n",
    "                    \n",
    "                    fold_mses = []\n",
    "                    \n",
    "                    for train_index, val_index in kf.split(X_poly):\n",
    "                        X_train, X_val = X_poly[train_index], X_poly[val_index]\n",
    "                        y_train, y_val = y_np[train_index], y_np[val_index]\n",
    "                        \n",
    "                        try:\n",
    "                            weights, bias, loss_history = bgd(\n",
    "                                X_train, y_train,\n",
    "                                learning_rate=learning_rate,\n",
    "                                batch_size=batch_size,\n",
    "                                reg_para=reg_para\n",
    "                            )\n",
    "                            \n",
    "                            y_pred = predict_batch_gd(X_val, weights, bias)\n",
    "                            \n",
    "                            # Scale predictions back\n",
    "                            y_pred = y_pred * y_scale\n",
    "                            y_val_original = y_val * y_scale\n",
    "                            \n",
    "                            mse = mean_squared_error(y_val_original, y_pred)\n",
    "                            fold_mses.append(mse)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error in fold: {str(e)}\")\n",
    "                            continue\n",
    "                    \n",
    "                    if fold_mses:  # only if we have valid results\n",
    "                        avg_mse = np.mean(fold_mses)\n",
    "                        \n",
    "                        results.append({\n",
    "                            'degree': degree,\n",
    "                            'batch_size': batch_size,\n",
    "                            'learning_rate': learning_rate,\n",
    "                            'reg_para': reg_para,\n",
    "                            'mse': avg_mse\n",
    "                        })\n",
    "                        \n",
    "                        if avg_mse < best_mse:\n",
    "                            best_mse = avg_mse\n",
    "                            best_params = {\n",
    "                                'degree': degree,\n",
    "                                'batch_size': batch_size,\n",
    "                                'learning_rate': learning_rate,\n",
    "                                'reg_para': reg_para\n",
    "                            }\n",
    "                            best_weights = weights * X_scale\n",
    "                            best_bias = bias * y_scale\n",
    "    \n",
    "    if best_params is None:\n",
    "        raise ValueError(\"No valid models found. Adjust hyperparameters.\")\n",
    "    \n",
    "    print(\"\\nBest parameters:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"{param}: {value}\")\n",
    "    print(f\"Best Mean Squared Error: {best_mse:.4f}\")\n",
    "    \n",
    "    return best_weights, best_bias, best_params, results\n",
    "\n",
    "def plot_training_history(loss_history):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(loss_history)\n",
    "    plt.title('Training Loss Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# running code\n",
    "# Get feature matrix X and target variable y\n",
    "X = standardizedData.drop('Rented Bike Count', axis=1)\n",
    "y = standardizedData['Rented Bike Count']\n",
    "\n",
    "print(\"Shape of data:\")\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "# Define the parameter grid \n",
    "param_grid = {\n",
    "    'degrees': [1, 2],  # Start with linear model\n",
    "    'batch_sizes': [16, 32, 64],  \n",
    "    'learning_rates': [0.00001, 0.0001, 0.001],  # Much smaller learning rates\n",
    "    'reg_parameters': [0.00001, 0.0001, 0.001]  # Smaller regularization parameters\n",
    "}\n",
    "\n",
    "# Perform grid search with cross validation\n",
    "try:\n",
    "    best_weights, best_bias, best_params, results = grid_search_bgd(\n",
    "        X, y,\n",
    "        folds=5,  # Reduced folds for faster testing\n",
    "        **param_grid\n",
    "    )\n",
    "\n",
    "    # Print results summary\n",
    "    print(\"\\nTraining Results Summary:\")\n",
    "    print(\"-------------------------\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"{param}: {value}\")\n",
    "\n",
    "    # Create predictions using best model\n",
    "    poly = PolynomialFeatures(degree=best_params['degree'], include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    predictions = predict_batch_gd(X_poly, best_weights, best_bias)\n",
    "\n",
    "    # Calculate and print final MSE\n",
    "    final_mse = mean_squared_error(y, predictions)\n",
    "    print(f\"\\nFinal MSE on full dataset: {final_mse:.4f}\")\n",
    "\n",
    "    # Plot actual vs predicted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y, predictions, alpha=0.5)\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual Bike Rentals')\n",
    "    plt.ylabel('Predicted Bike Rentals')\n",
    "    plt.title('Actual vs Predicted Bike Rentals')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "\"\"\"\n",
    "numpy .clip: https://numpy.org/doc/2.1/reference/generated/numpy.clip.html\n",
    "numpy .dot: https://numpy.org/doc/2.1/reference/generated/numpy.dot.html\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eb977b-f6bf-45f1-bb5d-eeec764bc3b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
